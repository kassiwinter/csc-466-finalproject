                                                                                                                                                                                                           CNET                                              For the most up-to-date news and information about the coronavirus pandemic, visit the WHO and CDC websites.                      YouTube is broadening its rules on medical misinformation to crack down on videos with false claims about all established vaccines, not just COVID-19 vaccines. The policy change paves the way for YouTube to possibly crimp further proliferation of persistent myths and conspiracies about vaccines, such as the untrue belief they cause autism or infertility.  YouTube, like Facebook, Twitter, Reddit and many other internet companies that give users a platform to post their own content, has grappled with how to balance freedom of expression with effective policing of the worst material posted on its site. Over the years, YouTube has grappled with different kinds of misinformation; conspiracy theories; discrimination; hate and harassment; child abuse and exploitation and videos of mass murder, all at an unprecedented global scale. Critics of YouTube argue that the company's content moderation efforts still fall short too often, or arrive too late.            Get the CNET Culture newsletter     Explore movies, games, superheroes and more with CNET Culture. Delivered Tuesdays and Fridays.         
Starting Wednesday, YouTube will take down any videos alleging that approved vaccines don't work, are dangerous, cause chronic health problems or contain harmful ingredients. The rule applies for vaccines approved and confirmed to be safe and effective by local health authorities and the World Health Organization.  "We've steadily seen false claims about the coronavirus vaccines spill over into misinformation about vaccines in general, and we're now at a point where it's more important than ever to expand the work we started with COVID-19 to other vaccines," the company said in a blog post.  YouTube has taken down 130,000 videos specifically for COVID-vaccine misinformation since last year, the company said Wednesday. To put that in context, YouTube has previously said it generally removes nearly 10 million total videos each quarter across the categories of policy violations. And since February 2020, YouTube has removed more than 1 million videos related to "dangerous" coronavirus information, such as false cures or claims of a hoax, the company said in August. (Putting these numbers in the greater context of YouTube's full scope is difficult because of the gargantuan scale of Google's service, which is the internet's biggest source of video, with more than 2 billion monthly users.) YouTube said the new policy applies to specific, established immunizations like vaccines for measles or Hepatitis B. But it also applies to false claims about established vaccines generally.  The new, broader policy will allow content about vaccine policies, new vaccine trials and historical vaccine successes or failures. YouTube said it would make exceptions for videos that include some misinformation about vaccines if they also include context about "countervailing views" from health authorities in the video itself, in its audio or in the title or description text.  Personal testimonials relating to vaccines will also be allowed, so long as the channel doesn't show a pattern of promoting vaccine hesitancy and the video doesn't violate any other of the site's rules.                                                                                               